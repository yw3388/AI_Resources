##cpu
https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel

cpu/cuda: conda install numba cudatoolkit pyculib

## from stackoverflow
RuntimeError: size mismatch, m1: [a x b], m2: [c x d]
all you have to care is b=c and you are done:

m1 is [a x b] which is [batch size x in features]

m2 is [c x d] which is [in features x out features]
https://pytorch.org/docs/master/generated/torch.nn.Linear.html#torch.nn.Linear


ValueError: attempted relative import beyond top-level package


##from website
class construction
assignment perfom on variables, objects in memory
function calls return objects

self.hunger = hunger
self.get_hunger(): return self.hunger

constructor: 
constructor overloading:
no parameter is no parameter
this.n = N(parameter)

this in front of the field name: this.number is not necessary. 

##cnn
convnet: width, height, depth
every layer has a simple api

##convolutional layer, pooling layer, fully-connected layer
INPUT: width, height, three channels
CONV: weight, regions connected to input, filters
relu: activation max(0, x)
pool: width, height
fc: fully connected layer


https://www.youtube.com/watch?v=Li5sVEXTIJw

Activation: gate to activate or not
softmax: to proba, then choose max to give output

Feed forward neural network--一种归一化

结构性数据与非结构性数据
convolution: 
a, b, 1
就view改成b， a， -1


torch.nn is nn.Module
apply fn recursively to every submodule
buffers: recurse is True


for name, buf in self.named_buffers():
  if name in ['running_var']:
  print(buf.size())
  
 for name, module in model.named_children():
    if name in ['conv4']:
      print(module)
      
  for name, param in named_parameters():
    if name in ['bias']:
        param.size()
        
        
        
        
###webscrapping and downloading data
Your bottleneck is probably that you write the file to disk first and then read it again (I/O). 
If the file does not exceed your machines random access memory, decompressing the file on the fly in memory might be a faster option
json.gzip



decompression from gzip pkg

read_csv(compression = 'gzip')


###decode, Stringio
Bytes flow:

.json() easy

shrimpy

##pandas.read_clipboard(sep='\\s+', **kwargs)[source]
Read text from clipboard and pass to read_csv.

Parameters
sepstr, default ‘s+’
A string or regex delimiter. The default of ‘s+’ denotes one or more whitespace characters.

**kwargs
See read_csv for the full argument list.

Returns
DataFrame
A parsed DataFrame object.


###DL
https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners
https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers
https://www.kaggle.com/arunkumarramanan/awesome-deep-learning-with-cnn-mnist-classifier
https://github.com/yw3388/awesome-deeplearning-resources

Blogs: 


/[+-]?\d+/ integers
/[+-]?\d+\./ 1., 321.
/[+-]?\.\d+/ .1, .234
/[+-]?\d+\.\d+/ 1.0

/[+-]?(\d+\.\d+|\d+\./
|\.\d+)/
